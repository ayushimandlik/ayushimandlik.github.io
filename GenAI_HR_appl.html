<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>FRBs</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="index.html" class="title">Home</a>
				<nav>
					<ul>
						<li><a href="resume.html">Resume</a></li>
					</ul>
				</nav>
			</header>

            <div style="background-color:#1DE9B6; color:white; padding:10px; text-align:center;">
                 <strong>Check out the <a href="https://huggingface.co/spaces/ayushimandlik09/HR_peoples_analytics" style="color:white; text-decoration:underline;" target="_blank">GenAI-Powered HR Assistant </a>! </strong>
 </div>
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="main" class="wrapper">
						<div class="inner">
                            <h1 class="major"> GenAI-Powered HR Assistant </h1>
<p>
                            In this project, I built a GenAI-powered HR assistant app that blends traditional data analysis with large language models to answer both factual and fuzzy employee-related queries. Using a synthetic HR dataset, the app supports interactive visual insights, performs code-driven aggregations, and responds conversationally using vector search and Gemini Pro. This hybrid approach ensures accurate, insightful, and flexible support for people analytics use cases. (A deeper dive into the dataset, methodology, and tools follows further down the page!)
                            </p>
                            <h2> üé¨ See the App in Action </h2>
                                    <p>
While the app runs smoothly most of the time, it does rely on internet connectivity, Hugging Face Spaces, and external APIs like Gemini‚Äîwhich can occasionally be unpredictable. To make sure you get a seamless experience no matter what, I‚Äôve created a series of video walkthroughs that showcase the app‚Äôs key features in action. It's a polished preview of everything the HR GenAI Assistant can do‚Äîminus the loading screens! üöÄüé•
</p>

<h3>Exploring Key Insights from the HR Dataset </h3>

<div style="text-align: center;">
<video id="myVideo" width=100% height='auto' controls="controls autoplay">
  <source src="images/insights_speed_up.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video><br>
</div>
<p>
This video walks through the interactive Insights Dashboard of the GenAI-powered HR Assistant. It starts with a correlation heatmap showing relationships between features with at least one correlation above 0.2. For instance, JobLevel and MonthlyIncome exhibit a strong positive correlation (0.95), while YearsWithCurrentManager and YearsAtCompany also track closely‚Äîrevealing tenure-linked management patterns. JobLevel and TotalWorkingYears also correlate significantly (0.77), indicating experience-driven promotion trends.
</p>
<p>
Next, count plots explore how attrition varies across categories like EnvironmentSatisfaction and MaritalStatus. One notable insight is that single employees tend to leave at a higher rate than married or divorced counterparts‚Äîpossibly indicating support systems influence retention.
</p>
<p>
Finally, pie charts illustrate the distribution of demographic and work-related categories like AgeGroup, JobRole, and EducationField, providing a snapshot of workforce composition. These visual summaries help HR teams identify patterns, target interventions, and develop data-backed strategies.
</p>


<h3>Answering Aggregation-Based Questions with GenAI </h3>

<div style="text-align: center;">
<video id="myVideo" width=100% height='auto' controls="controls autoplay">
  <source src="images/aggregation_query_clipped_sped_up.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video><br>
</div>
<p>
This video demonstrates how the assistant handles factual, metric-based queries like "How many employees left last year?" or "What is the average salary by department?" Behind the scenes, the app uses a language model to generate Python code, which is then executed securely on the backend to produce answers from the DataFrame.
</p> <p>
This approach offers both accuracy and flexibility‚Äîusers can ask free-form questions while still getting precise, real-time data. It‚Äôs especially powerful for non-technical HR staff who want quick answers without writing code or SQL. You‚Äôll see examples of queries, how fast the LLM responds, and the exact outputs it returns.
</p>

<h3>Fuzzy, Exploratory Questions via LLM + FAISS</h3>

<div style="text-align: center;">
<video id="myVideo" width=100% height='auto' controls="controls autoplay">
  <source src="images/fuzzy_query_sped_up.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video><br>
</div>


<p>
In this video, the assistant tackles exploratory or open-ended HR questions like "Why do people usually leave?" or "What makes a good performer?" These are handled using retrieval-augmented generation (RAG)‚Äîwhere relevant insights are pulled from HR policy or documentation and passed to the Gemini Pro LLM via LangChain and FAISS.
</p>
<p>
This allows the assistant to provide informed, context-aware answers even when the question isn‚Äôt directly tied to the dataset. For example, it can describe common traits of employees who leave, referencing trends or policy descriptions stored in the vector index. This makes the assistant not just a data tool, but a conversational HR knowledge companion.</p>

Now that you  have seen the app in action, let us dive into the tech that powers it ‚Äî starting with a quick look at what 'GenAI' actually is.

  <div style="margin-top: 30px;"></div>
                            <h1 class = "major">What is GenAI?</h1>
                                <p>Generative AI (GenAI) refers to a class of models capable of generating content such as text, code, or even visuals. These models are trained on vast datasets and can perform tasks like answering questions, writing scripts, and summarizing documents. In this app, GenAI is used to interpret user queries about HR data and respond intelligently‚Äîeither with natural language or generated Python code that calculates answers behind the scenes. To better understand how the app delivers intelligent responses, let us explore the core mechanisms that power it.</p>

                                <h2>A Peek Under the Hood </h2>
                                <h3> Embeddings</h3><p>
Embeddings are how the LLM ‚Äúunderstands‚Äù your HR data. I used the Hugging Face sentence-transformer model all-MiniLM-L6-v2, which converts each piece of text (like employee profiles, job descriptions, reasons for attrition) into a numerical vector. These vectors capture semantic meaning‚Äîso the assistant can find relevant context even if the exact words don‚Äôt match the user‚Äôs query. </p>

                                <h3> Vectorstore with FAISS</h3>
                                <p>Once the embeddings are generated, I store them in a FAISS (Facebook AI Similarity Search) index. FAISS is a high-performance library built for lightning-fast similarity search, even on large datasets. It‚Äôs ideal for querying the closest ‚Äúchunks‚Äù of information when answering fuzzy or retrieval-based questions.
</p>
<p> <em> Alternatives to FAISS:</em>  Chroma, Weaviate, and Pinecone (a fully managed vector DB). I chose FAISS because it‚Äôs lightweight and perfect for local/portable deployment. </p>
<h3>  LangChain: The Orchestration Layer </h3>
<p>

LangChain is what brings everything together. It handles connecting to the LLM (in my case, Gemini via Google Generative AI), prompt engineering, retrieval from the FAISS vectorstore, running chains (like RetrievalQA) to combine user input, context, and LLM responses.
</p>
<p> <em> Alternatives to Langchain:</em> You could also build similar workflows manually using tools like LLM API + Python scripts, or use Haystack (an open-source alternative to LangChain), but LangChain makes it very developer-friendly.
 </p>

 <h3>Hugging Face Spaces + Docker = Cloud Deployment </h3> 
 <p>
 To avoid running everything on my local machine (which was... slow), I deployed the app to Hugging Face Spaces using the Docker SDK. Hugging Face Spaces is a free hosting service for ML and GenAI apps. It supports apps built with Gradio, Streamlit, or even custom Docker containers. This way the heavy lifting happens in the cloud, users can try the app right from their browser and I do not have to share API keys or embeddings locally
 </p>
							<h1 class="major">Project Overview</h1>
                            <h3> The Dataset</h3>
                                <p>I used a synthetic HR dataset (1,470 rows √ó 36 columns) containing:</p>
    <ul>
      <li>Demographics: Age, Gender, Marital Status</li>
      <li>Job Details: Role, Department, Years at Company</li>
      <li>Satisfaction and Performance Scores</li>
      <li>Attrition Status: Whether an employee has left</li>
    </ul>

    <h3>Tools & Stack </h3>
    <table>
      <thead>
        <tr>
          <th>Tool</th>
          <th>Purpose</th>
        </tr>
      </thead>
      <tbody>
        <tr><td>Streamlit</td><td>Frontend interface</td></tr>
        <tr><td>LangChain</td><td>Prompt routing and model interaction</td></tr>
        <tr><td>Gemini Pro (ChatGoogleGenerativeAI)</td><td>LLM powering both query types</td></tr>
        <tr><td>FAISS</td><td>Vector-based semantic search</td></tr>
        <tr><td>Hugging Face Spaces</td><td>Deployment platform</td></tr>
        <tr><td>Python & Pandas</td><td>Data manipulation</td></tr>
        <tr><td>dotenv</td><td>Secure environment variable handling</td></tr>
      </tbody>
    </table>
    <h3>Alternatives:</h3>
    <ul>
      <li><strong>LLMs:</strong> GPT-4, Claude, Mistral</li>
      <li><strong>Vector Store:</strong> ChromaDB, Pinecone</li>
      <li><strong>Frontend:</strong> Gradio, Flask + React</li>
    </ul>

    <h2>Lessons Learned While Debugging </h2>
  <div>
    <h3>üöÄ Slow local loading of embedding models</h3>
    <p><strong>Solution:</strong> Cached models using <code>@st.cache_resource</code>, and saved embeddings and vectorstore locally improving dev speed and responsiveness..</p>
  </div>

  <div>
    <h3>üê¢ Heavy LLM calls slowing down the app</h3>
    <p><strong>Solution:</strong> Deployed to Hugging Face Spaces, freeing up local compute and boosting app performance.</p>
  </div>
  <div>
    <h3>üîê Security with API keys</h3>
    <p><strong>Solution:</strong> Used <code>.env</code> files locally and Hugging Face Secrets for deployment.</p>
  </div>
    
  <div>
    <h3>üîÑ Long response times for aggregation-based queries</h3>
    <p><strong>Solution:</strong> Separated aggregation and fuzzy query pipelines. For structured queries, I routed them through a custom code-gen pipeline that bypasses the LLM and executes Python directly for faster responses.</p> 
  </div>
    
<a href="https://huggingface.co/spaces/ayushimandlik09/HR_peoples_analytics" class="button  primary fit" target="_blank">GenAI-Powered HR Assistant
  </a>



                            <!-- <span class="image fit"><img src="images/ABC_pipeline.png" alt="" /></span> -->
						</div>
					</section>
			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
