<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>FRBs</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="index.html" class="title">Home</a>
				<nav>
					<ul>
						<li><a href="resume.html">Resume</a></li>
					</ul>
				</nav>
			</header>

            <div style="background-color:#1DE9B6; color:white; padding:10px; text-align:center;">
                 <strong>Check out the <a href="https://huggingface.co/spaces/ayushimandlik09/HR_peoples_analytics" style="color:white; text-decoration:underline;" target="_blank">GenAI-Powered HR Assistant </a>! </strong>
 </div>
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="main" class="wrapper">
						<div class="inner">

                            <h1 class="major">What is GenAI?</h1>
                                <p>Generative AI (GenAI) refers to a class of models capable of generating content such as text, code, or even visuals. These models are trained on vast datasets and can perform tasks like answering questions, writing scripts, and summarizing documents. In this app, GenAI is used to interpret user queries about HR data and respond intelligently‚Äîeither with natural language or generated Python code that calculates answers behind the scenes.</p>

                                <h2>A Peek Under the Hood </h2>
                                <h3> Embeddings</h3><p>
Embeddings are how the LLM ‚Äúunderstands‚Äù your HR data. I used the Hugging Face sentence-transformer model all-MiniLM-L6-v2, which converts each piece of text (like employee profiles, job descriptions, reasons for attrition) into a numerical vector. These vectors capture semantic meaning‚Äîso the assistant can find relevant context even if the exact words don‚Äôt match the user‚Äôs query. </p>

                                <h3> Vectorstore with FAISS</h3>
                                <p>Once the embeddings are generated, I store them in a FAISS (Facebook AI Similarity Search) index. FAISS is a high-performance library built for lightning-fast similarity search, even on large datasets. It‚Äôs ideal for querying the closest ‚Äúchunks‚Äù of information when answering fuzzy or retrieval-based questions.
</p>
<p> <em> Alternatives to FAISS:</em>  Chroma, Weaviate, and Pinecone (a fully managed vector DB). I chose FAISS because it‚Äôs lightweight and perfect for local/portable deployment. </p>
<h3>  LangChain: The Orchestration Layer </h3>
<p>

LangChain is what brings everything together. It handles connecting to the LLM (in my case, Gemini via Google Generative AI), prompt engineering, retrieval from the FAISS vectorstore, running chains (like RetrievalQA) to combine user input, context, and LLM responses.
</p>
<p> <em> Alternatives to Langchain:</em> You could also build similar workflows manually using tools like LLM API + Python scripts, or use Haystack (an open-source alternative to LangChain), but LangChain makes it very developer-friendly.
 </p>

 <h3>Hugging Face Spaces + Docker = Cloud Deployment </h3> 
 <p>
 To avoid running everything on my local machine (which was... slow), I deployed the app to Hugging Face Spaces using the Docker SDK. Hugging Face Spaces is a free hosting service for ML and GenAI apps. It supports apps built with Gradio, Streamlit, or even custom Docker containers. This way the heavy lifting happens in the cloud, users can try the app right from their browser and I do not have to share API keys or embeddings locally
 </p>
							<h1 class="major">Project Overview</h1>
                            <h3> The Dataset</h3>
                                <p>I used a synthetic HR dataset (1,470 rows √ó 36 columns) containing:</p>
    <ul>
      <li>Demographics: Age, Gender, Marital Status</li>
      <li>Job Details: Role, Department, Years at Company</li>
      <li>Satisfaction and Performance Scores</li>
      <li>Attrition Status: Whether an employee has left</li>
    </ul>

<h3> App Functionality: The Hybrid Approach </h3>
<em><b>1. Factual / Aggregation-Based Queries</b> </em>
    <p><strong>Example:</strong> "How many employees left last year?"</p>
    <p>LLM generates code ‚Üí Code is safely executed ‚Üí Result returned as text.</p>

    <b><em>2. Fuzzy / Natural Language Queries</b></em>
    <p><strong>Example:</strong> "What are some common traits of people who left?"</p>
    <p>Query ‚Üí FAISS retrieves context ‚Üí Gemini Pro generates a natural language summary.</p>
    
    <h3> üß∞ Tools & Stack </h3>
    <table>
      <thead>
        <tr>
          <th>Tool</th>
          <th>Purpose</th>
        </tr>
      </thead>
      <tbody>
        <tr><td>Streamlit</td><td>Frontend interface</td></tr>
        <tr><td>LangChain</td><td>Prompt routing and model interaction</td></tr>
        <tr><td>Gemini Pro (ChatGoogleGenerativeAI)</td><td>LLM powering both query types</td></tr>
        <tr><td>FAISS</td><td>Vector-based semantic search</td></tr>
        <tr><td>Hugging Face Spaces</td><td>Deployment platform</td></tr>
        <tr><td>Python & Pandas</td><td>Data manipulation</td></tr>
        <tr><td>dotenv</td><td>Secure environment variable handling</td></tr>
      </tbody>
    </table>
    <h3>üîÅ Alternatives</h3>
    <ul>
      <li><strong>LLMs:</strong> GPT-4, Claude, Mistral</li>
      <li><strong>Vector Store:</strong> ChromaDB, Pinecone</li>
      <li><strong>Frontend:</strong> Gradio, Flask + React</li>
    </ul>

    <h2> Bottlenecks & How I Overcame Them </h2>
  <div>
    <h3>üöÄ Slow local loading of embedding models</h3>
    <p><strong>Solution:</strong> Cached models using <code>@st.cache_resource</code>, and saved embeddings and vectorstore locally.</p>
  </div>

  <div>
    <h3>üìÇ FAISS index not loading due to relative paths</h3>
    <p><strong>Solution:</strong> Switched to absolute paths and verified persistence across sessions.</p>
  </div>

  <div>
    <h3>üê¢ Heavy LLM calls slowing down the app</h3>
    <p><strong>Solution:</strong> Deployed to Hugging Face Spaces, freeing up local compute.</p>
  </div>

  <div>
    <h3>üîê Security with API keys</h3>
    <p><strong>Solution:</strong> Used <code>.env</code> files locally and Hugging Face Secrets for deployment.</p>
  </div>
    
<a href="https://huggingface.co/spaces/ayushimandlik09/HR_peoples_analytics" class="button  primary fit" target="_blank">GenAI-Powered HR Assistant
  </a>



                            <!-- <span class="image fit"><img src="images/ABC_pipeline.png" alt="" /></span> -->
						</div>
					</section>
			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
